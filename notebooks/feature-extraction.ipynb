{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce5c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5377e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/processed/eda_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57633cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "agg_df = df.groupby('CustomerId').agg({\n",
    "    'Amount': ['sum', 'mean', 'std', 'max', 'min', 'count'],\n",
    "    'Value': ['sum', 'mean', 'std', 'max', 'min'],\n",
    "    'TransactionHour': 'nunique',\n",
    "    'TransactionDay': 'nunique',\n",
    "    'TransactionMonth': 'nunique'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7b3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten column names\n",
    "agg_df.columns = ['CustomerId'] + ['_'.join(col).strip() for col in agg_df.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512a8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [col for col in agg_df.columns if agg_df[col].dtype in ['int64', 'float64'] and col != 'CustomerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccf66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: ['Amount_sum', 'Amount_mean', 'Amount_std', 'Amount_max', 'Amount_min', 'Amount_count', 'Value_sum', 'Value_mean', 'Value_std', 'Value_max', 'Value_min', 'TransactionHour_nunique', 'TransactionDay_nunique', 'TransactionMonth_nunique']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = agg_df.drop(columns=['CustomerId'])\n",
    "y = np.zeros(X.shape[0]) \n",
    "\n",
    "print(\"X columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054239ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_features)\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_scaled = numeric_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86515dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Proxy labels created. High-risk cluster: 0\n",
      "is_high_risk\n",
      "0    2307\n",
      "1    1435\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# proxy labels creation\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'], errors='coerce')\n",
    "\n",
    "# ðŸ“… Define snapshot date for Recency calculation\n",
    "snapshot_date = df['TransactionStartTime'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# ðŸ§® Calculate RFM per CustomerId\n",
    "rfm = df.groupby('CustomerId').agg({\n",
    "    'TransactionStartTime': lambda x: (snapshot_date - x.max()).days,\n",
    "    'TransactionId': 'count',\n",
    "    'Value': 'sum'\n",
    "}).reset_index()\n",
    "rfm.columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# ðŸ“Š Visual check (optional)\n",
    "# sns.pairplot(rfm[['Recency', 'Frequency', 'Monetary']])\n",
    "\n",
    "# ðŸ§¼ Scale RFM for clustering\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\n",
    "\n",
    "# ðŸš€ KMeans Clustering (3 segments)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# ðŸ” Determine high-risk cluster: lowest Frequency + Monetary, highest Recency\n",
    "cluster_summary = rfm.groupby('Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': 'mean'\n",
    "}).sort_values(by='Frequency')\n",
    "\n",
    "high_risk_cluster = cluster_summary.index[0]  # assume lowest freq is highest risk\n",
    "rfm['is_high_risk'] = (rfm['Cluster'] == high_risk_cluster).astype(int)\n",
    "\n",
    "# ðŸ§© Save or merge this label with processed features\n",
    "# e.g. rfm[['CustomerId', 'is_high_risk']] \n",
    "#merge it with df\n",
    "rfm = rfm[['CustomerId', 'is_high_risk']]\n",
    "df = df.merge(rfm, on='CustomerId', how='left')\n",
    "\n",
    "# Save the processed data with proxy labels\n",
    "PROCESSED_DATA_PATH = \"../data/processed/eda_data_with_proxy_labels.csv\"\n",
    "df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "\n",
    "print(\"âœ… Proxy labels created. High-risk cluster:\", high_risk_cluster)\n",
    "print(rfm['is_high_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff94c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3742, 95662]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mis_high_risk\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ðŸ”€ Train-test split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ðŸ§ª Define models to evaluate\u001b[39;00m\n\u001b[32m     19\u001b[39m models = {\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m\"\u001b[39m: LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRandom Forest\u001b[39m\u001b[33m\"\u001b[39m: RandomForestClassifier(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     22\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Documents\\Kifya-AI\\credit-risk-probability-model-week5\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Documents\\Kifya-AI\\credit-risk-probability-model-week5\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2916\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Documents\\Kifya-AI\\credit-risk-probability-model-week5\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Documents\\Kifya-AI\\credit-risk-probability-model-week5\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [3742, 95662]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/eda_data_with_proxy_labels.csv\")  # Adjust path as needed\n",
    "\n",
    "df_final = pd.DataFrame(X_scaled)\n",
    "df_final['CustomerId'] = customer_ids.values\n",
    "df_final = df_final.merge(rfm[['CustomerId', 'is_high_risk']], on='CustomerId')\n",
    "\n",
    "y = df_final['is_high_risk']\n",
    "X_final = df_final.drop(columns=['CustomerId', 'is_high_risk'])\n",
    "\n",
    "# ðŸ”€ Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# ðŸ¤– Train and Evaluate (Task 5)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(\"credit-risk-model\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        prec = precision_score(y_test, preds)\n",
    "        rec = recall_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        roc = roc_auc_score(y_test, probs)\n",
    "\n",
    "        mlflow.log_param(\"model\", name)\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"roc_auc\": roc\n",
    "        })\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\", registered_model_name=name.replace(\" \", \"_\"))\n",
    "\n",
    "        print(f\"âœ… {name} Results\")\n",
    "        print(classification_report(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
